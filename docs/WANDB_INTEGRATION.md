# Weights & Biases (wandb) æ•´åˆæŒ‡å—

é€™æ˜¯ä¸€å€‹å®Œæ•´çš„æ•™å­¸ï¼Œèªªæ˜å¦‚ä½•å°‡ `wandb` å°å…¥æ‚¨çš„ PyTorch å°ˆæ¡ˆï¼Œä»¥æ•æ‰æ‰€æœ‰è¨“ç·´å’Œæ¨ç†æŒ‡æ¨™ã€‚

-----

## ğŸš€ æ­¥é©Ÿ 1: å®‰è£èˆ‡ç™»å…¥ (HPC ç’°å¢ƒ)

### 1. å®‰è£å‡½å¼åº«

åœ¨æ‚¨çš„ HPC ç’°å¢ƒä¸­ (å¯èƒ½æ˜¯åœ¨æ‚¨çš„ Slurm è…³æœ¬æˆ–äº’å‹•å¼ç¯€é»ä¸­)ï¼Œç¢ºä¿ `wandb` å·²å®‰è£ï¼š

```bash
pip install wandb
```

æˆ–è€…ä½¿ç”¨ `uv` (æ¨è–¦ï¼Œèˆ‡å°ˆæ¡ˆä¸€è‡´)ï¼š

```bash
uv add wandb
```

### 2. ç™»å…¥ (HPC æœ€ä½³å¯¦è¸)

ç”±æ–¼ HPC ç¯€é»é€šå¸¸æ²’æœ‰ç€è¦½å™¨ï¼Œæ‚¨éœ€è¦ä½¿ç”¨ API Key ç™»å…¥ã€‚

1. åˆ° [wandb.ai/authorize](https://wandb.ai/authorize) ç²å–æ‚¨çš„ API Keyã€‚

2. åœ¨æ‚¨çš„ HPC ç™»å…¥ç¯€é»ä¸ŠåŸ·è¡Œï¼š

   ```bash
   wandb login
   ```

   ç•¶å®ƒæç¤ºæ™‚ï¼Œè²¼ä¸Šæ‚¨çš„ API Keyã€‚

3. **(æ¨è–¦)** ç‚ºäº†è®“æ‚¨çš„ Slurm ä»»å‹™è‡ªå‹•ç™»å…¥ï¼Œæœ€å¥½çš„æ–¹æ³•æ˜¯å°‡ API Key è¨­ç½®ç‚ºç’°å¢ƒè®Šæ•¸ã€‚åœ¨æ‚¨çš„ `.bashrc` æˆ–æäº¤è…³æœ¬ä¸­åŠ å…¥ï¼š

   ```bash
   export WANDB_API_KEY="YOUR_API_KEY_HERE"
   ```

-----

## ğŸ› ï¸ æ­¥é©Ÿ 2: æ•´åˆåˆ°æ‚¨çš„ PyTorch è…³æœ¬

`wandb` çš„æ ¸å¿ƒæ˜¯ `wandb.init()` å’Œ `wandb.log()`ã€‚

- `wandb.init()`ï¼šåœ¨è…³æœ¬é–‹å§‹æ™‚èª¿ç”¨ä¸€æ¬¡ï¼Œç”¨æ–¼åˆå§‹åŒ–ä¸€å€‹æ–°çš„ã€ŒRunã€ã€‚
- `wandb.log()`ï¼šåœ¨è¨“ç·´/é©—è­‰è¿´åœˆä¸­èª¿ç”¨ï¼Œç”¨æ–¼è¨˜éŒ„æŒ‡æ¨™ã€‚
- `wandb.summary`ï¼šç”¨æ–¼å„²å­˜**æœ€çµ‚**çš„å–®ä¸€å€¼æŒ‡æ¨™ (ä¾‹å¦‚ã€ŒAvg. F1ã€æˆ–ã€ŒPeak VRAMã€)ã€‚
- `wandb.Artifact`ï¼šç”¨æ–¼å„²å­˜æ¨¡å‹æ¬Šé‡æˆ–è³‡æ–™é›†ã€‚

### ç¯„ä¾‹è…³æœ¬çµæ§‹ (Pseudo-code)

é€™æ˜¯ä¸€å€‹å®Œæ•´çš„ç¯„ä¾‹ï¼Œå±•ç¤ºå¦‚ä½•æ•´åˆæ‚¨æ‰€æœ‰çš„æŒ‡æ¨™ã€‚

```python
import wandb
import torch
import time
import os
from torch.utils.data import DataLoader
from your_model_file import YourModel # æ›¿æ›æˆæ‚¨çš„æ¨¡å‹
from your_dataset_file import YourDataset # æ›¿æ›æˆæ‚¨çš„è³‡æ–™é›†
from your_metrics_file import calculate_accuracy, calculate_f1 # æ›¿æ›æˆæ‚¨çš„è¨ˆç®—å‡½å¼

# --- 1. å®šç¾©æ‚¨çš„å¯¦é©—é…ç½® ---
# é€™äº›é…ç½®æœƒè¢« wandb è¨˜éŒ„ï¼Œä¸¦è®“æ‚¨èƒ½å¤ åˆ†çµ„å’Œéæ¿¾
config = {
    "model_variant": "QLoRA", # 'Full LoRA', 'LoRA+PTQ'
    "quantization_strategy": "int4", # 'None', 'int8'
    "dataset": "LongRefiner_Combined",
    "epochs": 5,
    "learning_rate": 1e-4,
    "batch_size": 8,
}

# --- 2. åˆå§‹åŒ– W&B Run ---
# (ç¢ºä¿ WANDB_API_KEY å·²åœ¨ç’°å¢ƒä¸­è¨­ç½®)
run = wandb.init(
    project="Your_Project_Name",  # æ‚¨çš„å°ˆæ¡ˆåç¨±
    config=config,               # ä¸Šæ–¹å®šç¾©çš„é…ç½®
    name=f"{config['model_variant']}_{config['dataset']}_run_{int(time.time())}", # Run çš„é¡¯ç¤ºåç¨±
    job_type="training"          # å°‡æ­¤ Run æ¨™è¨˜ç‚º "training"
)

# ä½¿ç”¨ wandb.config è¨ªå•é…ç½® (é€™æ˜¯ä¸€ç¨®æœ€ä½³å¯¦è¸)
cfg = wandb.config

# --- 3. æº–å‚™æ¨¡å‹ã€è³‡æ–™å’Œå„ªåŒ–å™¨ ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = YourModel(config=cfg).to(device)
train_dataset = YourDataset(split='train')
val_dataset = YourDataset(split='validation')
train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size)
val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size)
optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)

# (å¯é¸) è¿½è¹¤æ¨¡å‹çš„æ¢¯åº¦å’Œæ‹“æ’²
# wandb.watch(model, log='all', log_freq=100) # æ¯ 100 æ­¥è¨˜éŒ„ä¸€æ¬¡

# --- 4. è¨“ç·´è¿´åœˆ ---
# ç”¨æ–¼è¨ˆç®— FLOPs (é€™æ˜¯ä¸€å€‹ç°¡åŒ–çš„ä¾‹å­ï¼Œæ‚¨éœ€è¦ä¸€å€‹æ›´æº–ç¢ºçš„ä¼°ç®—å™¨)
# ç¯„ä¾‹: ä½¿ç”¨ä¸€å€‹ library æˆ–æ‰‹å‹•è¨ˆç®—
# å‡è¨­: model.get_flops_per_step() è¿”å› TFLOPs
# TFLOPs_per_step = model.get_flops_per_step(cfg.batch_size) 

TFLOPs_per_step = 1.2 # å‡è¨­å€¼ (TFLOPs)

# é‡ç½® VRAM è¿½è¹¤å™¨
if torch.cuda.is_available():
    torch.cuda.reset_max_memory_allocated()

total_steps = 0
global_step = 0
print(f"--- Starting Training for {cfg.model_variant} ---")

for epoch in range(cfg.epochs):
    epoch_start_time = time.time()
    model.train()
    
    total_train_samples = 0
    
    for step, batch in enumerate(train_loader):
        batch = {k: v.to(device) for k, v in batch.items()}
        
        # --- å‰å‘å‚³æ’­ ---
        outputs = model(**batch)
        loss = outputs.loss
        
        # --- åå‘å‚³æ’­ ---
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        # --- è¨˜éŒ„ Step-level æŒ‡æ¨™ ---
        if step % 20 == 0: # æ¯ 20 æ­¥è¨˜éŒ„ä¸€æ¬¡
            wandb.log({
                "train/loss": loss.item(),
                "epoch": epoch,
                "step": global_step
            })
            
        total_train_samples += len(batch['input_ids'])
        total_steps += 1
        global_step += 1
    
    # --- è¨˜éŒ„ Epoch-level è¨“ç·´æŒ‡æ¨™ ---
    epoch_end_time = time.time()
    epoch_duration_sec = epoch_end_time - epoch_start_time
    epoch_duration_min = epoch_duration_sec / 60.0
    
    # d. System-Level: Training Metrics
    throughput = total_train_samples / epoch_duration_sec
    
    wandb.log({
        "train/epoch_time_min": epoch_duration_min,
        "train/throughput_samples_per_sec": throughput,
        "epoch": epoch
    })
    
    # --- 5. é©—è­‰è¿´åœˆ ---
    model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch in val_loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model.generate(**batch) # å‡è¨­
            
            # (è’é›†æ‚¨çš„é æ¸¬å’Œæ¨™ç±¤)
            # all_preds.extend(decode(outputs))
            # all_labels.extend(decode(batch['labels']))
            pass 
    
    # c. Task-Level Metrics
    # (å‡è¨­æ‚¨æœ‰ all_preds å’Œ all_labels)
    # val_accuracy = calculate_accuracy(all_preds, all_labels)
    # val_f1 = calculate_f1(all_preds, all_labels)
    
    # ç¯„ä¾‹å‡è³‡æ–™
    val_accuracy = 0.8 + (epoch / cfg.epochs) * 0.1 # å‡è³‡æ–™
    val_f1 = 0.75 + (epoch / cfg.epochs) * 0.1     # å‡è³‡æ–™
    
    print(f"Epoch {epoch}: Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}")
    wandb.log({
        "validation/accuracy": val_accuracy,
        "validation/f1_score": val_f1,
        "epoch": epoch
    })

# --- 6. è¨“ç·´çµæŸ - è¨˜éŒ„æœ€çµ‚æŒ‡æ¨™ (Summary) ---
print("--- Training Finished. Logging final summary metrics. ---")

# d. System-Level: Training Metrics
if torch.cuda.is_available():
    peak_vram_gb = torch.cuda.max_memory_allocated() / (1024**3)
    wandb.summary["train/peak_vram_gb"] = peak_vram_gb
    print(f"Peak Training VRAM: {peak_vram_gb:.2f} GB")

# å‡è¨­ #GPUs = 1 (åœ¨ HPC ä¸Šæ‚¨å¯èƒ½éœ€è¦å¾ os.environ['SLURM_NTASKS'] æˆ– torch.cuda.device_count() ç²å–)
num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1
total_training_flops_tflops = TFLOPs_per_step * total_steps * num_gpus
wandb.summary["train/total_training_flops_tflops"] = total_training_flops_tflops

# c. Task-Level Metrics (å‡è¨­æˆ‘å€‘é—œå¿ƒçš„æ˜¯æœ€å¾Œä¸€å€‹ epoch çš„è¡¨ç¾)
wandb.summary["final/validation_accuracy"] = val_accuracy
wandb.summary["final/validation_f1_score"] = val_f1

# (å¦‚æœæ‚¨è¨ˆç®—äº†æ‰€æœ‰ dataset çš„å¹³å‡ F1ï¼Œåœ¨é€™è£¡è¨˜éŒ„)
# wandb.summary["final/avg_f1"] = avg_f1_all_datasets

# --- 7. ä¿å­˜æ¨¡å‹ (ä½¿ç”¨ W&B Artifacts) ---
# é€™æ˜¯è¿½è¹¤ã€ŒModel Sizeã€çš„æœ€ä½³æ–¹å¼
model_path = f"model_{cfg.model_variant}.pth"
torch.save(model.state_dict(), model_path)

# å‰µå»ºä¸€å€‹ Artifact
artifact = wandb.Artifact(
    name=f"model-{cfg.model_variant}", # Artifact çš„åç¨±
    type="model",                      # é¡å‹
    metadata=cfg                       # é™„åŠ å…ƒæ•¸æ“š
)
artifact.add_file(model_path) # å°‡æ¨¡å‹æ–‡ä»¶åŠ å…¥
wandb.log_artifact(artifact) # ä¸Šå‚³ Artifact

# d. System-Level: Inference Metrics (Model Size)
model_size_gb = os.path.getsize(model_path) / (1024**3)
wandb.summary["inference/model_size_gb"] = model_size_gb
print(f"Model Size: {model_size_gb:.2f} GB")

# çµæŸ Run
run.finish()
```

-----

## ğŸ“Š æ­¥é©Ÿ 3: è¨˜éŒ„ Inference æŒ‡æ¨™

æ‚¨çš„ Inference æŒ‡æ¨™ (Latency, VRAM) æ‡‰è©²åœ¨ä¸€å€‹**å–®ç¨çš„è…³æœ¬**ä¸­æ¸¬é‡ï¼Œä¸¦è¨˜éŒ„åˆ°ä¸€å€‹**æ–°çš„ `wandb` Run** ä¸­ã€‚

```python
# inference_benchmark.py
import wandb
import torch
import time
from your_model_file import YourModel
from your_dataset_file import YourDataset

# --- 1. åˆå§‹åŒ–ä¸€å€‹ "inference" Run ---
run = wandb.init(
    project="Your_Project_Name",
    job_type="inference", # æ¨™è¨˜ç‚º "inference"
    name="Inference_Benchmark_QLoRA_int4"
)

# --- 2. ä¸‹è¼‰æ¨¡å‹ (å¦‚æœä½¿ç”¨ Artifacts) ---
# é€™æ˜¯å¾ W&B ä¸‹è¼‰æ¨¡å‹çš„ç¯„ä¾‹
# artifact = run.use_artifact('Your_Project_Name/model-QLoRA:latest', type='model')
# artifact_dir = artifact.download()
# model_path = os.path.join(artifact_dir, "model_QLoRA.pth")

# (è¼‰å…¥æ¨¡å‹...)
device = torch.device("cuda")
model = YourModel(...)
# model.load_state_dict(torch.load(model_path))
model.to(device)
model.eval()

# (è¼‰å…¥æ‚¨çš„æŸ¥è©¢è³‡æ–™)
# query_dataset = ...

# --- 3. æ¸¬é‡ Latency å’Œ VRAM ---
latencies_ms = []

if torch.cuda.is_available():
    torch.cuda.reset_max_memory_allocated()

with torch.no_grad():
    for query in query_dataset:
        query = query.to(device)
        
        # é ç†± (Warmup)
        # for _ in range(5):
        #     _ = model.generate(query)
            
        # æ¸¬é‡æ™‚é–“
        start_time = time.time()
        _ = model.generate(query)
        end_time = time.time()
        
        latency_ms = (end_time - start_time) * 1000
        latencies_ms.append(latency_ms)

# --- 4. è¨˜éŒ„åˆ° wandb.summary ---
avg_latency_ms = sum(latencies_ms) / len(latencies_ms)
wandb.summary["inference/avg_latency_ms_per_sample"] = avg_latency_ms

if torch.cuda.is_available():
    peak_inference_vram_gb = torch.cuda.max_memory_allocated() / (1024**3)
    wandb.summary["inference/peak_vram_gb"] = peak_inference_vram_gb

# (FLOPs per Query é€šå¸¸æ˜¯ç†è«–å€¼ï¼Œæ‚¨å¯ä»¥ç›´æ¥è¨˜éŒ„)
# GFLOPs_per_query = model.get_flops_per_query(query) / 1e9
wandb.summary["inference/flops_per_query_gflops"] = 50.0 # å‡è¨­å€¼

print(f"Avg Latency: {avg_latency_ms:.2f} ms")
print(f"Peak Inference VRAM: {peak_inference_vram_gb:.2f} GB")

run.finish()
```

-----

## ğŸ“ˆ æ­¥é©Ÿ 4: åœ¨ `wandb` å„€è¡¨æ¿ä¸Šå¯¦ç¾æ‚¨çš„åˆ†æ (e, f)

é€™ä¸€æ­¥**ä¸éœ€è¦å¯«ç¨‹å¼**ï¼Œå…¨éƒ½åœ¨ `wandb` ç¶²é ä»‹é¢ä¸Šå®Œæˆï¼š

### 1. Convergence Curves (e.4)

- **æ–¹æ³•:** é€™æœƒè‡ªå‹•ç”Ÿæˆã€‚`wandb` æœƒè‡ªå‹•ç¹ªè£½æ‚¨ `wandb.log()` çš„æ‰€æœ‰æŒ‡æ¨™ (å¦‚ `validation/f1_score`, `train/loss`) å° `step` æˆ– `epoch` çš„æ›²ç·šã€‚
- **æ¯”è¼ƒ:** åœ¨æ‚¨çš„å°ˆæ¡ˆé é¢ï¼Œ`wandb` æœƒè‡ªå‹•å°‡æ‰€æœ‰ Run çš„åœ–è¡¨ç–ŠåŠ åœ¨ä¸€èµ·ï¼Œæ‚¨å¯ä»¥è¼•é¬†æ¯”è¼ƒ `QLoRA` å’Œ `Full LoRA` çš„ `validation/f1_score` æ›²ç·šã€‚

### 2. Comparative Analysis (e.1, e.2)

- **æ–¹æ³•:** å„€è¡¨æ¿é ‚éƒ¨æœ‰ä¸€å€‹è¡¨æ ¼ã€‚é»æ“Š "Columns" æŒ‰éˆ•ï¼Œæ·»åŠ æ‚¨è¨˜éŒ„çš„ `summary` æŒ‡æ¨™ï¼Œä¾‹å¦‚ï¼š
  - `final/validation_f1_score`
  - `train/peak_vram_gb`
  - `inference/avg_latency_ms_per_sample`
  - `inference/model_size_gb`
- **åˆ†çµ„:** é»æ“Š "Group"ï¼Œé¸æ“‡ `config.model_variant`ã€‚`wandb` ç¾åœ¨æœƒå°‡ `QLoRA`, `Full LoRA` ç­‰è‡ªå‹•åˆ†çµ„ï¼Œä¸¦é¡¯ç¤ºæ¯çµ„çš„å¹³å‡æŒ‡æ¨™ã€‚é€™èƒ½è®“æ‚¨ä¸€ç›®äº†ç„¶åœ°çœ‹åˆ°ã€ŒPerformance Retentionã€å’Œã€ŒEfficiency Gainsã€ã€‚

### 3. Pareto Frontier Visualization (e.3)

- **æ–¹æ³•:** åœ¨æ‚¨çš„å°ˆæ¡ˆé é¢ï¼Œé»æ“Š "Add panel" (æˆ– "+" åœ–ç¤º)ï¼Œé¸æ“‡ **"Scatter Plot"**ã€‚
- **è¨­ç½®:**
  - **X-axis:** é¸æ“‡ `inference/avg_latency_ms_per_sample` (æˆ– `train/peak_vram_gb`)ã€‚
  - **Y-axis:** é¸æ“‡ `final/validation_f1_score`ã€‚
  - **Color (é¡è‰²):** é¸æ“‡ `config.model_variant`ã€‚
- **çµæœ:** æ‚¨æœƒå¾—åˆ°ä¸€å¼µåœ–ï¼Œé¡¯ç¤ºæ‰€æœ‰å¯¦é©—çš„ã€Œæ•ˆç‡ vs æº–ç¢ºåº¦ã€æ¬Šè¡¡ï¼Œæ¯å€‹é»çš„é¡è‰²ä»£è¡¨å®ƒæ˜¯å“ªå€‹æ¨¡å‹è®Šé«”ã€‚é€™å°±æ˜¯æ‚¨çš„ Pareto Frontierã€‚

é€éé€™å¥—æµç¨‹ï¼Œæ‚¨åœ¨è…³æœ¬ä¸­è¨˜éŒ„çš„æ¯é …æ•¸æ“šéƒ½æœƒç›´æ¥å°æ‡‰åˆ°æ‚¨å ±å‘Šä¸­éœ€è¦çš„åˆ†æåœ–è¡¨ã€‚

-----

## ğŸ”— ç›¸é—œè³‡æº

- [wandb å®˜æ–¹æ–‡æª”](https://docs.wandb.ai/)
- [wandb æˆæ¬Šé é¢](https://wandb.ai/authorize)
- [wandb Python API åƒè€ƒ](https://docs.wandb.ai/ref/python)

